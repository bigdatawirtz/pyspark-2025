{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Structured API - Getting Started Guide\n",
    "\n",
    "## Introduction to DataFrames\n",
    "\n",
    "DataFrames are the core of Spark's Structured API. Think of them as distributed tables with named columns and schemas, similar to pandas DataFrames but distributed across a cluster.\n",
    "\n",
    "**Note**: In PySpark, we primarily work with DataFrames. Datasets are a Scala/Java feature only.\n",
    "\n",
    "## 1. Creating Your First DataFrame\n",
    "\n",
    "### From Python Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+--------------+\n",
      "| id|   name|age|           job|\n",
      "+---+-------+---+--------------+\n",
      "|  1|  Alice| 25|      Engineer|\n",
      "|  2|    Bob| 30|Data Scientist|\n",
      "|  3|Charlie| 35|       Manager|\n",
      "|  4|  Diana| 28|      Engineer|\n",
      "|  5|    Eve| 32|Data Scientist|\n",
      "+---+-------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "#spark = SparkSession.builder \\\n",
    "#    .appName(\"StructuredAPIDemo\") \\\n",
    "#    .getOrCreate()\n",
    "\n",
    "# From a list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 25, \"Engineer\"),\n",
    "    (2, \"Bob\", 30, \"Data Scientist\"),\n",
    "    (3, \"Charlie\", 35, \"Manager\"),\n",
    "    (4, \"Diana\", 28, \"Engineer\"),\n",
    "    (5, \"Eve\", 32, \"Data Scientist\")\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"age\", \"job\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|age| id|   name|\n",
      "+---+---+-------+\n",
      "| 25|  1|  Alice|\n",
      "| 30|  2|    Bob|\n",
      "| 35|  3|Charlie|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create pandas DataFrame\n",
    "pandas_df = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35]\n",
    "})\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.show()\n",
    "\n",
    "# Convert back to pandas (for small datasets)\n",
    "result_pandas = spark_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading Data from Files\n",
    "\n",
    "### CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV with options\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"sample_data/data.csv\")\n",
    "\n",
    "# Alternative syntax\n",
    "df = spark.read.csv(\"./sample_data/data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# With explicit schema (better for production)\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"./sample_data/data.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 25|\n",
      "|  2|    Bob| 30|\n",
      "|  3|Charlie| 35|\n",
      "|  4|  Diana| 28|\n",
      "|  5|    Eve| 32|\n",
      "|  6|  Frank| 45|\n",
      "|  7|  Grace| 29|\n",
      "|  8|  Henry| 38|\n",
      "|  9|    Ivy| 27|\n",
      "| 10|   Jack| 41|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---+-------+------+\n",
      "|age| department| id|   name|salary|\n",
      "+---+-----------+---+-------+------+\n",
      "| 25|Engineering|  1|  Alice| 80000|\n",
      "| 30|  Marketing|  2|    Bob| 65000|\n",
      "| 35|Engineering|  3|Charlie| 90000|\n",
      "| 28|      Sales|  4|  Diana| 70000|\n",
      "| 32|Engineering|  5|    Eve| 85000|\n",
      "| 45|  Marketing|  6|  Frank| 75000|\n",
      "| 29|      Sales|  7|  Grace| 72000|\n",
      "| 38|Engineering|  8|  Henry| 95000|\n",
      "| 27|  Marketing|  9|    Ivy| 68000|\n",
      "| 41|      Sales| 10|   Jack| 78000|\n",
      "+---+-----------+---+-------+------+\n",
      "\n",
      "+----------+--------+------+----------+--------+\n",
      "|      date|order_id| price|   product|quantity|\n",
      "+----------+--------+------+----------+--------+\n",
      "|2024-01-18|       4| 350.0|   Monitor|       1|\n",
      "|2024-01-19|       5| 89.99|    Webcam|       4|\n",
      "|2024-01-20|       6| 120.0|Headphones|       2|\n",
      "|2024-01-15|       1|1200.0|    Laptop|       2|\n",
      "|2024-01-16|       2|  25.0|     Mouse|       5|\n",
      "|2024-01-17|       3|  75.0|  Keyboard|       3|\n",
      "+----------+--------+------+----------+--------+\n",
      "\n",
      "+---+-----+--------+\n",
      "| id|price| product|\n",
      "+---+-----+--------+\n",
      "|  3|  500|  Tablet|\n",
      "|  4|  350| Monitor|\n",
      "|  1| 1200|  Laptop|\n",
      "|  2|  800|   Phone|\n",
      "|  5|   75|Keyboard|\n",
      "|  6|   25|   Mouse|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading JSON\n",
    "df = spark.read.json(\"./sample_data/data.json\")\n",
    "df.show()\n",
    "\n",
    "# Reading multiple JSON files\n",
    "df = spark.read.json([\"./sample_data/file1.json\", \"./sample_data/file2.json\"])\n",
    "df.show()\n",
    "\n",
    "# Reading from directory\n",
    "df = spark.read.json(\"./sample_data/data_folder/\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parquet Files (Recommended for Big Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading Parquet\n",
    "df = spark.read.parquet(\"./sample_data/data.parquet\")\n",
    "df.show()\n",
    "\n",
    "# Parquet is columnar and compressed - best for analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|Apache Spark is a...|\n",
      "|It provides high-...|\n",
      "|Spark runs on Had...|\n",
      "|It can access div...|\n",
      "|PySpark is the Py...|\n",
      "|DataFrames are th...|\n",
      "|RDDs are the fund...|\n",
      "|Spark provides bu...|\n",
      "+--------------------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading text files (one line per row)\n",
    "df = spark.read.text(\"sample_data/data.txt\")\n",
    "df.show()\n",
    "\n",
    "# Reading ORC\n",
    "df = spark.read.orc(\"sample_data/data.orc\")\n",
    "df.show()\n",
    "\n",
    "# Reading from databases (requires JDBC driver)\n",
    "#df = spark.read \\\n",
    "#    .format(\"jdbc\") \\\n",
    "#    .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\") \\\n",
    "#    .option(\"dbtable\", \"users\") \\\n",
    "#    .option(\"user\", \"username\") \\\n",
    "#    .option(\"password\", \"password\") \\\n",
    "#    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "+---+-------+---+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "|id |name   |age|department |salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|1  |Alice  |25 |Engineering|80000 |\n",
      "|2  |Bob    |30 |Marketing  |65000 |\n",
      "|3  |Charlie|35 |Engineering|90000 |\n",
      "|4  |Diana  |28 |Sales      |70000 |\n",
      "|5  |Eve    |32 |Engineering|85000 |\n",
      "|6  |Frank  |45 |Marketing  |75000 |\n",
      "|7  |Grace  |29 |Sales      |72000 |\n",
      "|8  |Henry  |38 |Engineering|95000 |\n",
      "|9  |Ivy    |27 |Marketing  |68000 |\n",
      "|10 |Jack   |41 |Sales      |78000 |\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "['id', 'name', 'age', 'department', 'salary']\n",
      "StructType(List(StructField(id,IntegerType,true),StructField(name,StringType,true),StructField(age,IntegerType,true),StructField(department,StringType,true),StructField(salary,IntegerType,true)))\n",
      "10\n",
      "+-------+------------------+-----+------------------+-----------+-----------------+\n",
      "|summary|                id| name|               age| department|           salary|\n",
      "+-------+------------------+-----+------------------+-----------+-----------------+\n",
      "|  count|                10|   10|                10|         10|               10|\n",
      "|   mean|               5.5| null|              33.0|       null|          77800.0|\n",
      "| stddev|3.0276503540974917| null|6.5659052011974035|       null|9795.690662508467|\n",
      "|    min|                 1|Alice|                25|Engineering|            65000|\n",
      "|    max|                10| Jack|                45|      Sales|            95000|\n",
      "+-------+------------------+-----+------------------+-----------+-----------------+\n",
      "\n",
      "+-------+------------------+-----+------------------+-----------+-----------------+\n",
      "|summary|                id| name|               age| department|           salary|\n",
      "+-------+------------------+-----+------------------+-----------+-----------------+\n",
      "|  count|                10|   10|                10|         10|               10|\n",
      "|   mean|               5.5| null|              33.0|       null|          77800.0|\n",
      "| stddev|3.0276503540974917| null|6.5659052011974035|       null|9795.690662508467|\n",
      "|    min|                 1|Alice|                25|Engineering|            65000|\n",
      "|    25%|                 3| null|                28|       null|            70000|\n",
      "|    50%|                 5| null|                30|       null|            75000|\n",
      "|    75%|                 8| null|                38|       null|            85000|\n",
      "|    max|                10| Jack|                45|      Sales|            95000|\n",
      "+-------+------------------+-----+------------------+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show first 10 rows\n",
    "df.show()\n",
    "\n",
    "# Show specific number of rows\n",
    "df.show(5)\n",
    "\n",
    "# Show without truncating long strings\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n",
    "# Output:\n",
    "# root\n",
    "#  |-- id: integer (nullable = true)\n",
    "#  |-- name: string (nullable = true)\n",
    "#  |-- age: integer (nullable = true)\n",
    "\n",
    "# Get column names\n",
    "print(df.columns)  # ['id', 'name', 'age', ...]\n",
    "\n",
    "# Get schema\n",
    "print(df.schema)\n",
    "\n",
    "# Count rows\n",
    "print(df.count())  # 5\n",
    "\n",
    "# Get summary statistics\n",
    "df.describe().show()\n",
    "df.summary().show()  # More detailed than describe()\n",
    "\n",
    "# First few rows as list of Row objects\n",
    "rows = df.head(3)\n",
    "first_row = df.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting and Projecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "|  Diana| 28|\n",
      "|    Eve| 32|\n",
      "|  Frank| 45|\n",
      "|  Grace| 29|\n",
      "|  Henry| 38|\n",
      "|    Ivy| 27|\n",
      "|   Jack| 41|\n",
      "+-------+---+\n",
      "\n",
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 25|\n",
      "|    Bob| 30|\n",
      "|Charlie| 35|\n",
      "|  Diana| 28|\n",
      "|    Eve| 32|\n",
      "|  Frank| 45|\n",
      "|  Grace| 29|\n",
      "|  Henry| 38|\n",
      "|    Ivy| 27|\n",
      "|   Jack| 41|\n",
      "+-------+---+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+-------+---+----------+\n",
      "|   name|age|age_plus_5|\n",
      "+-------+---+----------+\n",
      "|  Alice| 25|        30|\n",
      "|    Bob| 30|        35|\n",
      "|Charlie| 35|        40|\n",
      "|  Diana| 28|        33|\n",
      "|    Eve| 32|        37|\n",
      "|  Frank| 45|        50|\n",
      "|  Grace| 29|        34|\n",
      "|  Henry| 38|        43|\n",
      "|    Ivy| 27|        32|\n",
      "|   Jack| 41|        46|\n",
      "+-------+---+----------+\n",
      "\n",
      "+-------+----------+\n",
      "|   name|double_age|\n",
      "+-------+----------+\n",
      "|  Alice|        50|\n",
      "|    Bob|        60|\n",
      "|Charlie|        70|\n",
      "|  Diana|        56|\n",
      "|    Eve|        64|\n",
      "|  Frank|        90|\n",
      "|  Grace|        58|\n",
      "|  Henry|        76|\n",
      "|    Ivy|        54|\n",
      "|   Jack|        82|\n",
      "+-------+----------+\n",
      "\n",
      "+-------------+------------+\n",
      "|employee_name|employee_age|\n",
      "+-------------+------------+\n",
      "|        Alice|          25|\n",
      "|          Bob|          30|\n",
      "|      Charlie|          35|\n",
      "|        Diana|          28|\n",
      "|          Eve|          32|\n",
      "|        Frank|          45|\n",
      "|        Grace|          29|\n",
      "|        Henry|          38|\n",
      "|          Ivy|          27|\n",
      "|         Jack|          41|\n",
      "+-------------+------------+\n",
      "\n",
      "+-------+---+-----------------+\n",
      "|   name|age|age_in_five_years|\n",
      "+-------+---+-----------------+\n",
      "|  Alice| 25|               30|\n",
      "|    Bob| 30|               35|\n",
      "|Charlie| 35|               40|\n",
      "|  Diana| 28|               33|\n",
      "|    Eve| 32|               37|\n",
      "|  Frank| 45|               50|\n",
      "|  Grace| 29|               34|\n",
      "|  Henry| 38|               43|\n",
      "|    Ivy| 27|               32|\n",
      "|   Jack| 41|               46|\n",
      "+-------+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Select specific columns\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Using col()\n",
    "df.select(col(\"name\"), col(\"age\")).show()\n",
    "\n",
    "# Select all columns\n",
    "df.select(\"*\").show()\n",
    "\n",
    "# Select with expressions\n",
    "df.select(\n",
    "    col(\"name\"),\n",
    "    col(\"age\"),\n",
    "    (col(\"age\") + 5).alias(\"age_plus_5\")\n",
    ").show()\n",
    "\n",
    "# Using expr() for SQL-like expressions\n",
    "df.select(\n",
    "    expr(\"name\"),\n",
    "    expr(\"age * 2 as double_age\")\n",
    ").show()\n",
    "\n",
    "# Select and rename\n",
    "df.select(\n",
    "    col(\"name\").alias(\"employee_name\"),\n",
    "    col(\"age\").alias(\"employee_age\")\n",
    ").show()\n",
    "\n",
    "# SelectExpr - SQL expressions\n",
    "df.selectExpr(\n",
    "    \"name\",\n",
    "    \"age\",\n",
    "    \"age + 5 as age_in_five_years\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-----+---+-----------+------+\n",
      "| id| name|age| department|salary|\n",
      "+---+-----+---+-----------+------+\n",
      "|  1|Alice| 25|Engineering| 80000|\n",
      "|  2|  Bob| 30|  Marketing| 65000|\n",
      "|  4|Diana| 28|      Sales| 70000|\n",
      "|  7|Grace| 29|      Sales| 72000|\n",
      "|  9|  Ivy| 27|  Marketing| 68000|\n",
      "+---+-----+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+----+---+----------+------+\n",
      "| id|name|age|department|salary|\n",
      "+---+----+---+----------+------+\n",
      "+---+----+---+----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-----+---+-----------+------+\n",
      "| id| name|age| department|salary|\n",
      "+---+-----+---+-----------+------+\n",
      "|  1|Alice| 25|Engineering| 80000|\n",
      "+---+-----+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Simple filter\n",
    "df.filter(col(\"age\") > 28).show()\n",
    "\n",
    "# Alternative: using where (same as filter)\n",
    "df.where(col(\"age\") > 28).show()\n",
    "\n",
    "# Using SQL string\n",
    "df.filter(\"age > 28\").show()\n",
    "\n",
    "# Multiple conditions - AND\n",
    "df.filter((col(\"age\") > 25) & (col(\"department\") == \"Engineering\")).show()\n",
    "\n",
    "# Multiple conditions - OR\n",
    "df.filter((col(\"age\") > 30) | (col(\"department\") == \"Engineering\")).show()\n",
    "\n",
    "# NOT\n",
    "df.filter(~(col(\"age\") > 30)).show()\n",
    "\n",
    "# IN operator\n",
    "df.filter(col(\"department\").isin([\"Engineering\", \"Sales\"])).show()\n",
    "\n",
    "# NULL checks\n",
    "df.filter(col(\"age\").isNull()).show()\n",
    "df.filter(col(\"age\").isNotNull()).show()\n",
    "\n",
    "# String operations\n",
    "df.filter(col(\"name\").startswith(\"A\")).show()\n",
    "df.filter(col(\"name\").endswith(\"e\")).show()\n",
    "df.filter(col(\"name\").contains(\"li\")).show()\n",
    "\n",
    "# Chaining filters\n",
    "df.filter(col(\"age\") > 25) \\\n",
    "  .filter(col(\"department\") == \"Engineering\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adding and Modifying Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+-------+\n",
      "| id|   name|age| department|salary|country|\n",
      "+---+-------+---+-----------+------+-------+\n",
      "|  1|  Alice| 25|Engineering| 80000|    USA|\n",
      "|  2|    Bob| 30|  Marketing| 65000|    USA|\n",
      "|  3|Charlie| 35|Engineering| 90000|    USA|\n",
      "|  4|  Diana| 28|      Sales| 70000|    USA|\n",
      "|  5|    Eve| 32|Engineering| 85000|    USA|\n",
      "|  6|  Frank| 45|  Marketing| 75000|    USA|\n",
      "|  7|  Grace| 29|      Sales| 72000|    USA|\n",
      "|  8|  Henry| 38|Engineering| 95000|    USA|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|    USA|\n",
      "| 10|   Jack| 41|      Sales| 78000|    USA|\n",
      "+---+-------+---+-----------+------+-------+\n",
      "\n",
      "+---+-------+---+-----------+------+------------+\n",
      "| id|   name|age| department|salary|age_category|\n",
      "+---+-------+---+-----------+------+------------+\n",
      "|  1|  Alice| 25|Engineering| 80000|       Young|\n",
      "|  2|    Bob| 30|  Marketing| 65000|      Middle|\n",
      "|  3|Charlie| 35|Engineering| 90000|      Middle|\n",
      "|  4|  Diana| 28|      Sales| 70000|       Young|\n",
      "|  5|    Eve| 32|Engineering| 85000|      Middle|\n",
      "|  6|  Frank| 45|  Marketing| 75000|      Senior|\n",
      "|  7|  Grace| 29|      Sales| 72000|       Young|\n",
      "|  8|  Henry| 38|Engineering| 95000|      Middle|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|       Young|\n",
      "| 10|   Jack| 41|      Sales| 78000|      Senior|\n",
      "+---+-------+---+-----------+------+------------+\n",
      "\n",
      "+---+-------+---+-----------+------+----------------+\n",
      "| id|   name|age| department|salary|estimated_salary|\n",
      "+---+-------+---+-----------+------+----------------+\n",
      "|  1|  Alice| 25|Engineering| 80000|           80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|           70000|\n",
      "|  3|Charlie| 35|Engineering| 90000|           80000|\n",
      "|  4|  Diana| 28|      Sales| 70000|           70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|           80000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|           70000|\n",
      "|  7|  Grace| 29|      Sales| 72000|           70000|\n",
      "|  8|  Henry| 38|Engineering| 95000|           80000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|           70000|\n",
      "| 10|   Jack| 41|      Sales| 78000|           70000|\n",
      "+---+-------+---+-----------+------+----------------+\n",
      "\n",
      "+---+-------------+---+-----------+------+\n",
      "| id|employee_name|age| department|salary|\n",
      "+---+-------------+---+-----------+------+\n",
      "|  1|        Alice| 25|Engineering| 80000|\n",
      "|  2|          Bob| 30|  Marketing| 65000|\n",
      "|  3|      Charlie| 35|Engineering| 90000|\n",
      "|  4|        Diana| 28|      Sales| 70000|\n",
      "|  5|          Eve| 32|Engineering| 85000|\n",
      "|  6|        Frank| 45|  Marketing| 75000|\n",
      "|  7|        Grace| 29|      Sales| 72000|\n",
      "|  8|        Henry| 38|Engineering| 95000|\n",
      "|  9|          Ivy| 27|  Marketing| 68000|\n",
      "| 10|         Jack| 41|      Sales| 78000|\n",
      "+---+-------------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 80000|\n",
      "|  2|    Bob| 30| 65000|\n",
      "|  3|Charlie| 35| 90000|\n",
      "|  4|  Diana| 28| 70000|\n",
      "|  5|    Eve| 32| 85000|\n",
      "|  6|  Frank| 45| 75000|\n",
      "|  7|  Grace| 29| 72000|\n",
      "|  8|  Henry| 38| 95000|\n",
      "|  9|    Ivy| 27| 68000|\n",
      "| 10|   Jack| 41| 78000|\n",
      "+---+-------+---+------+\n",
      "\n",
      "+---+-------+------+\n",
      "| id|   name|salary|\n",
      "+---+-------+------+\n",
      "|  1|  Alice| 80000|\n",
      "|  2|    Bob| 65000|\n",
      "|  3|Charlie| 90000|\n",
      "|  4|  Diana| 70000|\n",
      "|  5|    Eve| 85000|\n",
      "|  6|  Frank| 75000|\n",
      "|  7|  Grace| 72000|\n",
      "|  8|  Henry| 95000|\n",
      "|  9|    Ivy| 68000|\n",
      "| 10|   Jack| 78000|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, when, col\n",
    "\n",
    "# Add a new column with constant value\n",
    "df_with_country = df.withColumn(\"country\", lit(\"USA\"))\n",
    "df_with_country.show()\n",
    "\n",
    "# Add column based on calculation\n",
    "df_with_age_category = df.withColumn(\n",
    "    \"age_category\",\n",
    "    when(col(\"age\") < 30, \"Young\")\n",
    "    .when(col(\"age\") < 40, \"Middle\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "df_with_age_category.show()\n",
    "\n",
    "# Multiple when conditions\n",
    "df_with_salary = df.withColumn(\n",
    "    \"estimated_salary\",\n",
    "    when(col(\"department\") == \"Engineering\", 80000)\n",
    "    .when(col(\"department\") == \"Data Scientist\", 90000)\n",
    "    .when(col(\"department\") == \"Manager\", 100000)\n",
    "    .otherwise(70000)\n",
    ")\n",
    "df_with_salary.show()\n",
    "\n",
    "# Rename column\n",
    "df_renamed = df.withColumnRenamed(\"name\", \"employee_name\")\n",
    "df_renamed.show()\n",
    "\n",
    "# Drop column\n",
    "df_dropped = df.drop(\"department\")\n",
    "df_dropped.show()\n",
    "\n",
    "# Drop multiple columns\n",
    "df_dropped_multi = df.drop(\"department\", \"age\")\n",
    "df_dropped_multi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, asc, desc\n",
    "\n",
    "# Sort ascending (default)\n",
    "df.orderBy(\"age\").show()\n",
    "df.sort(\"age\").show()  # same as orderBy\n",
    "\n",
    "# Sort descending\n",
    "df.orderBy(col(\"age\").desc()).show()\n",
    "df.orderBy(desc(\"age\")).show()\n",
    "\n",
    "# Multiple columns\n",
    "df.orderBy(col(\"department\"), col(\"age\").desc()).show()\n",
    "\n",
    "# Using asc and desc\n",
    "df.orderBy(asc(\"department\"), desc(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grouping and Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|      Sales|    3|\n",
      "|Engineering|    4|\n",
      "|  Marketing|    3|\n",
      "+-----------+-----+\n",
      "\n",
      "+-----------+-----+------------------+-------+-------+\n",
      "| department|count|           avg_age|min_age|max_age|\n",
      "+-----------+-----+------------------+-------+-------+\n",
      "|      Sales|    3|32.666666666666664|     28|     41|\n",
      "|Engineering|    4|              32.5|     25|     38|\n",
      "|  Marketing|    3|              34.0|     27|     45|\n",
      "+-----------+-----+------------------+-------+-------+\n",
      "\n",
      "+-----------+---------------+\n",
      "|total_count|overall_avg_age|\n",
      "+-----------+---------------+\n",
      "|         10|           33.0|\n",
      "+-----------+---------------+\n",
      "\n",
      "+-----------+-------+-----+--------+\n",
      "|   category|product|price|quantity|\n",
      "+-----------+-------+-----+--------+\n",
      "|Electronics| Laptop| 1000|       2|\n",
      "|Electronics|  Phone|  500|       5|\n",
      "|Electronics| Tablet|  300|       3|\n",
      "|   Clothing|  Shirt|   50|      10|\n",
      "|   Clothing|  Pants|   80|       7|\n",
      "|   Clothing|  Shoes|  100|       4|\n",
      "+-----------+-------+-----+--------+\n",
      "\n",
      "+-----------+------------+-------------+-----------------+\n",
      "|   category|num_products|total_revenue|        avg_price|\n",
      "+-----------+------------+-------------+-----------------+\n",
      "|Electronics|           3|         5400|            600.0|\n",
      "|   Clothing|           3|         1460|76.66666666666667|\n",
      "+-----------+------------+-------------+-----------------+\n",
      "\n",
      "+-----------+-----------------------+\n",
      "|category   |products               |\n",
      "+-----------+-----------------------+\n",
      "|Electronics|[Laptop, Phone, Tablet]|\n",
      "|Clothing   |[Shirt, Pants, Shoes]  |\n",
      "+-----------+-----------------------+\n",
      "\n",
      "+-------------+\n",
      "|distinct_jobs|\n",
      "+-------------+\n",
      "|            3|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, min, max, collect_list, countDistinct\n",
    "\n",
    "# Simple groupBy with count\n",
    "df.groupBy(\"department\").count().show()\n",
    "\n",
    "# Multiple aggregations\n",
    "df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"count\"),\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    min(\"age\").alias(\"min_age\"),\n",
    "    max(\"age\").alias(\"max_age\")\n",
    ").show()\n",
    "\n",
    "# Without groupBy - aggregating entire DataFrame\n",
    "df.agg(\n",
    "    count(\"*\").alias(\"total_count\"),\n",
    "    avg(\"age\").alias(\"overall_avg_age\")\n",
    ").show()\n",
    "\n",
    "# Example with more data\n",
    "sales_data = [\n",
    "    (\"Electronics\", \"Laptop\", 1000, 2),\n",
    "    (\"Electronics\", \"Phone\", 500, 5),\n",
    "    (\"Electronics\", \"Tablet\", 300, 3),\n",
    "    (\"Clothing\", \"Shirt\", 50, 10),\n",
    "    (\"Clothing\", \"Pants\", 80, 7),\n",
    "    (\"Clothing\", \"Shoes\", 100, 4)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(\n",
    "    sales_data, \n",
    "    [\"category\", \"product\", \"price\", \"quantity\"]\n",
    ")\n",
    "\n",
    "sales_df.show()\n",
    "\n",
    "# Group by category\n",
    "category_stats = sales_df.groupBy(\"category\").agg(\n",
    "    count(\"product\").alias(\"num_products\"),\n",
    "    sum(col(\"price\") * col(\"quantity\")).alias(\"total_revenue\"),\n",
    "    avg(\"price\").alias(\"avg_price\")\n",
    ")\n",
    "category_stats.show()\n",
    "\n",
    "# Collect list of items\n",
    "products_by_category = sales_df.groupBy(\"category\").agg(\n",
    "    collect_list(\"product\").alias(\"products\")\n",
    ")\n",
    "products_by_category.show(truncate=False)\n",
    "\n",
    "# Count distinct\n",
    "df.agg(countDistinct(\"department\").alias(\"distinct_jobs\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n",
      "|emp_id|   name|dept_id|\n",
      "+------+-------+-------+\n",
      "|     1|  Alice|    101|\n",
      "|     2|    Bob|    102|\n",
      "|     3|Charlie|    101|\n",
      "|     4|  Diana|    103|\n",
      "+------+-------+-------+\n",
      "\n",
      "+-------+-----------+\n",
      "|dept_id|  dept_name|\n",
      "+-------+-----------+\n",
      "|    101|Engineering|\n",
      "|    102|  Marketing|\n",
      "|    103|      Sales|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------+------+-------+-----------+\n",
      "|dept_id|emp_id|   name|  dept_name|\n",
      "+-------+------+-------+-----------+\n",
      "|    103|     4|  Diana|      Sales|\n",
      "|    101|     3|Charlie|Engineering|\n",
      "|    101|     1|  Alice|Engineering|\n",
      "|    102|     2|    Bob|  Marketing|\n",
      "+-------+------+-------+-----------+\n",
      "\n",
      "+------+-------+-------+-------+-----------+\n",
      "|emp_id|   name|dept_id|dept_id|  dept_name|\n",
      "+------+-------+-------+-------+-----------+\n",
      "|     4|  Diana|    103|    103|      Sales|\n",
      "|     1|  Alice|    101|    101|Engineering|\n",
      "|     3|Charlie|    101|    101|Engineering|\n",
      "|     2|    Bob|    102|    102|  Marketing|\n",
      "+------+-------+-------+-------+-----------+\n",
      "\n",
      "+-------+------+-------+-----------+\n",
      "|dept_id|emp_id|   name|  dept_name|\n",
      "+-------+------+-------+-----------+\n",
      "|    103|     4|  Diana|      Sales|\n",
      "|    101|     3|Charlie|Engineering|\n",
      "|    101|     1|  Alice|Engineering|\n",
      "|    102|     2|    Bob|  Marketing|\n",
      "+-------+------+-------+-----------+\n",
      "\n",
      "+-------+------+-------+-----------+\n",
      "|dept_id|emp_id|   name|  dept_name|\n",
      "+-------+------+-------+-----------+\n",
      "|    103|     4|  Diana|      Sales|\n",
      "|    101|     1|  Alice|Engineering|\n",
      "|    101|     3|Charlie|Engineering|\n",
      "|    102|     2|    Bob|  Marketing|\n",
      "+-------+------+-------+-----------+\n",
      "\n",
      "+-------+------+-------+-----------+\n",
      "|dept_id|emp_id|   name|  dept_name|\n",
      "+-------+------+-------+-----------+\n",
      "|    103|     4|  Diana|      Sales|\n",
      "|    101|     3|Charlie|Engineering|\n",
      "|    101|     1|  Alice|Engineering|\n",
      "|    102|     2|    Bob|  Marketing|\n",
      "+-------+------+-------+-----------+\n",
      "\n",
      "+-------+-------+---+\n",
      "|   name|   name|age|\n",
      "+-------+-------+---+\n",
      "|  Alice|  Alice| 25|\n",
      "|    Bob|    Bob| 30|\n",
      "|Charlie|Charlie| 35|\n",
      "|  Diana|  Diana| 28|\n",
      "|    Eve|    Eve| 32|\n",
      "|  Frank|  Frank| 45|\n",
      "|  Grace|  Grace| 29|\n",
      "|  Henry|  Henry| 38|\n",
      "|    Ivy|    Ivy| 27|\n",
      "|   Jack|   Jack| 41|\n",
      "+-------+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data for joins\n",
    "employees = [\n",
    "    (1, \"Alice\", 101),\n",
    "    (2, \"Bob\", 102),\n",
    "    (3, \"Charlie\", 101),\n",
    "    (4, \"Diana\", 103)\n",
    "]\n",
    "\n",
    "departments = [\n",
    "    (101, \"Engineering\"),\n",
    "    (102, \"Marketing\"),\n",
    "    (103, \"Sales\")\n",
    "]\n",
    "\n",
    "emp_df = spark.createDataFrame(employees, [\"emp_id\", \"name\", \"dept_id\"])\n",
    "dept_df = spark.createDataFrame(departments, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "emp_df.show()\n",
    "dept_df.show()\n",
    "\n",
    "# Inner join (default)\n",
    "result = emp_df.join(dept_df, \"dept_id\")\n",
    "result.show()\n",
    "\n",
    "# Explicit join condition\n",
    "result = emp_df.join(dept_df, emp_df.dept_id == dept_df.dept_id)\n",
    "result.show()\n",
    "\n",
    "# Left join\n",
    "result = emp_df.join(dept_df, \"dept_id\", \"left\")\n",
    "result.show()\n",
    "\n",
    "# Right join\n",
    "result = emp_df.join(dept_df, \"dept_id\", \"right\")\n",
    "result.show()\n",
    "\n",
    "# Outer join\n",
    "result = emp_df.join(dept_df, \"dept_id\", \"outer\")\n",
    "result.show()\n",
    "\n",
    "# Self join\n",
    "df_aliased = df.alias(\"df1\")\n",
    "df2_aliased = df.alias(\"df2\")\n",
    "self_join = df_aliased.join(\n",
    "    df2_aliased, \n",
    "    col(\"df1.age\") == col(\"df2.age\")\n",
    ").select(\"df1.name\", \"df2.name\", \"df1.age\")\n",
    "self_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Working with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+----------------+------+\n",
      "|text            |lowercase       |uppercase       |trimmed         |length|\n",
      "+----------------+----------------+----------------+----------------+------+\n",
      "|  Hello World   |  hello world   |  HELLO WORLD   |Hello World     |15    |\n",
      "|PYSPARK         |pyspark         |PYSPARK         |PYSPARK         |7     |\n",
      "|Data Engineering|data engineering|DATA ENGINEERING|Data Engineering|16    |\n",
      "+----------------+----------------+----------------+----------------+------+\n",
      "\n",
      "+-------+\n",
      "|first_5|\n",
      "+-------+\n",
      "|    Hel|\n",
      "|  PYSPA|\n",
      "|  Data |\n",
      "+-------+\n",
      "\n",
      "+--------------------+\n",
      "|            name_job|\n",
      "+--------------------+\n",
      "| Alice - Engineering|\n",
      "|     Bob - Marketing|\n",
      "|Charlie - Enginee...|\n",
      "|       Diana - Sales|\n",
      "|   Eve - Engineering|\n",
      "|   Frank - Marketing|\n",
      "|       Grace - Sales|\n",
      "| Henry - Engineering|\n",
      "|     Ivy - Marketing|\n",
      "|        Jack - Sales|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|            combined|\n",
      "+--------------------+\n",
      "| Alice | Engineering|\n",
      "|     Bob | Marketing|\n",
      "|Charlie | Enginee...|\n",
      "|       Diana | Sales|\n",
      "|   Eve | Engineering|\n",
      "|   Frank | Marketing|\n",
      "|       Grace | Sales|\n",
      "| Henry | Engineering|\n",
      "|     Ivy | Marketing|\n",
      "|        Jack | Sales|\n",
      "+--------------------+\n",
      "\n",
      "+----------------------+\n",
      "|words                 |\n",
      "+----------------------+\n",
      "|[, , Hello, World, , ]|\n",
      "|[PYSPARK]             |\n",
      "|[Data, Engineering]   |\n",
      "+----------------------+\n",
      "\n",
      "+----------------+\n",
      "|replaced        |\n",
      "+----------------+\n",
      "|  Hi World      |\n",
      "|PYSPARK         |\n",
      "|Data Engineering|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    lower, upper, trim, ltrim, rtrim, \n",
    "    concat, concat_ws, substring, length,\n",
    "    split, regexp_replace\n",
    ")\n",
    "\n",
    "text_data = [\n",
    "    (1, \"  Hello World  \"),\n",
    "    (2, \"PYSPARK\"),\n",
    "    (3, \"Data Engineering\")\n",
    "]\n",
    "\n",
    "text_df = spark.createDataFrame(text_data, [\"id\", \"text\"])\n",
    "\n",
    "# String transformations\n",
    "result = text_df.select(\n",
    "    col(\"text\"),\n",
    "    lower(col(\"text\")).alias(\"lowercase\"),\n",
    "    upper(col(\"text\")).alias(\"uppercase\"),\n",
    "    trim(col(\"text\")).alias(\"trimmed\"),\n",
    "    length(col(\"text\")).alias(\"length\")\n",
    ")\n",
    "result.show(truncate=False)\n",
    "\n",
    "# Substring (1-indexed)\n",
    "text_df.select(substring(col(\"text\"), 1, 5).alias(\"first_5\")).show()\n",
    "\n",
    "# Concatenate\n",
    "df.select(\n",
    "    concat(col(\"name\"), lit(\" - \"), col(\"department\")).alias(\"name_job\")\n",
    ").show()\n",
    "\n",
    "# Concatenate with separator\n",
    "df.select(\n",
    "    concat_ws(\" | \", col(\"name\"), col(\"department\")).alias(\"combined\")\n",
    ").show()\n",
    "\n",
    "# Split string\n",
    "text_df.select(\n",
    "    split(col(\"text\"), \" \").alias(\"words\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# Replace\n",
    "text_df.select(\n",
    "    regexp_replace(col(\"text\"), \"Hello\", \"Hi\").alias(\"replaced\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Working with Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+-----------+\n",
      "|      date|year|month|day|day_of_week|\n",
      "+----------+----+-----+---+-----------+\n",
      "|2024-01-15|2024|    1| 15|          2|\n",
      "|2024-02-20|2024|    2| 20|          3|\n",
      "|2024-03-10|2024|    3| 10|          1|\n",
      "+----------+----+-----+---+-----------+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|      date| next_week| last_week|\n",
      "+----------+----------+----------+\n",
      "|2024-01-15|2024-01-22|2024-01-08|\n",
      "|2024-02-20|2024-02-27|2024-02-13|\n",
      "|2024-03-10|2024-03-17|2024-03-03|\n",
      "+----------+----------+----------+\n",
      "\n",
      "+----------+-----------------------+\n",
      "|today     |now                    |\n",
      "+----------+-----------------------+\n",
      "|2026-01-28|2026-01-28 17:39:06.285|\n",
      "+----------+-----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2024-01-15|\n",
      "|2024-02-20|\n",
      "+----------+\n",
      "\n",
      "+--------+\n",
      "|days_ago|\n",
      "+--------+\n",
      "|     744|\n",
      "|     708|\n",
      "|     689|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    current_date, current_timestamp, date_add, date_sub,\n",
    "    year, month, dayofmonth, dayofweek, hour, minute,\n",
    "    to_date, to_timestamp, datediff, months_between\n",
    ")\n",
    "\n",
    "# Create sample date data\n",
    "from datetime import datetime, date\n",
    "\n",
    "date_data = [\n",
    "    (1, date(2024, 1, 15), datetime(2024, 1, 15, 10, 30, 0)),\n",
    "    (2, date(2024, 2, 20), datetime(2024, 2, 20, 14, 45, 30)),\n",
    "    (3, date(2024, 3, 10), datetime(2024, 3, 10, 9, 15, 45))\n",
    "]\n",
    "\n",
    "date_df = spark.createDataFrame(date_data, [\"id\", \"date\", \"timestamp\"])\n",
    "\n",
    "# Extract date parts\n",
    "date_df.select(\n",
    "    col(\"date\"),\n",
    "    year(col(\"date\")).alias(\"year\"),\n",
    "    month(col(\"date\")).alias(\"month\"),\n",
    "    dayofmonth(col(\"date\")).alias(\"day\"),\n",
    "    dayofweek(col(\"date\")).alias(\"day_of_week\")\n",
    ").show()\n",
    "\n",
    "# Date arithmetic\n",
    "date_df.select(\n",
    "    col(\"date\"),\n",
    "    date_add(col(\"date\"), 7).alias(\"next_week\"),\n",
    "    date_sub(col(\"date\"), 7).alias(\"last_week\")\n",
    ").show()\n",
    "\n",
    "# Current date and timestamp\n",
    "df.select(\n",
    "    current_date().alias(\"today\"),\n",
    "    current_timestamp().alias(\"now\")\n",
    ").show(1, truncate=False)\n",
    "\n",
    "# Parse string to date\n",
    "string_dates = [(\"2024-01-15\",), (\"2024-02-20\",)]\n",
    "string_df = spark.createDataFrame(string_dates, [\"date_string\"])\n",
    "string_df.select(\n",
    "    to_date(col(\"date_string\"), \"yyyy-MM-dd\").alias(\"date\")\n",
    ").show()\n",
    "\n",
    "# Date difference\n",
    "date_df.select(\n",
    "    datediff(current_date(), col(\"date\")).alias(\"days_ago\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Window Functions\n",
    "\n",
    "Window functions perform calculations across rows related to the current row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+\n",
      "|  month|   category|sales|\n",
      "+-------+-----------+-----+\n",
      "|2024-01|Electronics| 1000|\n",
      "|2024-01|   Clothing|  500|\n",
      "|2024-01|       Food|  300|\n",
      "|2024-02|Electronics| 1200|\n",
      "|2024-02|   Clothing|  600|\n",
      "|2024-02|       Food|  400|\n",
      "|2024-03|Electronics|  900|\n",
      "|2024-03|   Clothing|  700|\n",
      "|2024-03|       Food|  350|\n",
      "+-------+-----------+-----+\n",
      "\n",
      "+-------+-----------+-----+-------+\n",
      "|  month|   category|sales|row_num|\n",
      "+-------+-----------+-----+-------+\n",
      "|2024-01|       Food|  300|      1|\n",
      "|2024-02|       Food|  400|      2|\n",
      "|2024-03|       Food|  350|      3|\n",
      "|2024-01|Electronics| 1000|      1|\n",
      "|2024-02|Electronics| 1200|      2|\n",
      "|2024-03|Electronics|  900|      3|\n",
      "|2024-01|   Clothing|  500|      1|\n",
      "|2024-02|   Clothing|  600|      2|\n",
      "|2024-03|   Clothing|  700|      3|\n",
      "+-------+-----------+-----+-------+\n",
      "\n",
      "+-------+-----------+-----+----+\n",
      "|  month|   category|sales|rank|\n",
      "+-------+-----------+-----+----+\n",
      "|2024-02|Electronics| 1200|   1|\n",
      "|2024-02|   Clothing|  600|   2|\n",
      "|2024-02|       Food|  400|   3|\n",
      "|2024-03|Electronics|  900|   1|\n",
      "|2024-03|   Clothing|  700|   2|\n",
      "|2024-03|       Food|  350|   3|\n",
      "|2024-01|Electronics| 1000|   1|\n",
      "|2024-01|   Clothing|  500|   2|\n",
      "|2024-01|       Food|  300|   3|\n",
      "+-------+-----------+-----+----+\n",
      "\n",
      "+-------+-----------+-----+----------+\n",
      "|  month|   category|sales|dense_rank|\n",
      "+-------+-----------+-----+----------+\n",
      "|2024-02|Electronics| 1200|         1|\n",
      "|2024-02|   Clothing|  600|         2|\n",
      "|2024-02|       Food|  400|         3|\n",
      "|2024-03|Electronics|  900|         1|\n",
      "|2024-03|   Clothing|  700|         2|\n",
      "|2024-03|       Food|  350|         3|\n",
      "|2024-01|Electronics| 1000|         1|\n",
      "|2024-01|   Clothing|  500|         2|\n",
      "|2024-01|       Food|  300|         3|\n",
      "+-------+-----------+-----+----------+\n",
      "\n",
      "+-------+-----------+-----+-------------+\n",
      "|  month|   category|sales|running_total|\n",
      "+-------+-----------+-----+-------------+\n",
      "|2024-01|       Food|  300|          300|\n",
      "|2024-02|       Food|  400|          700|\n",
      "|2024-03|       Food|  350|         1050|\n",
      "|2024-01|Electronics| 1000|         1000|\n",
      "|2024-02|Electronics| 1200|         2200|\n",
      "|2024-03|Electronics|  900|         3100|\n",
      "|2024-01|   Clothing|  500|          500|\n",
      "|2024-02|   Clothing|  600|         1100|\n",
      "|2024-03|   Clothing|  700|         1800|\n",
      "+-------+-----------+-----+-------------+\n",
      "\n",
      "+-------+-----------+-----+----------------+----------------+\n",
      "|  month|   category|sales|prev_month_sales|next_month_sales|\n",
      "+-------+-----------+-----+----------------+----------------+\n",
      "|2024-01|       Food|  300|            null|             400|\n",
      "|2024-02|       Food|  400|             300|             350|\n",
      "|2024-03|       Food|  350|             400|            null|\n",
      "|2024-01|Electronics| 1000|            null|            1200|\n",
      "|2024-02|Electronics| 1200|            1000|             900|\n",
      "|2024-03|Electronics|  900|            1200|            null|\n",
      "|2024-01|   Clothing|  500|            null|             600|\n",
      "|2024-02|   Clothing|  600|             500|             700|\n",
      "|2024-03|   Clothing|  700|             600|            null|\n",
      "+-------+-----------+-----+----------------+----------------+\n",
      "\n",
      "+-------+-----------+-----+------------------+\n",
      "|  month|   category|sales|        moving_avg|\n",
      "+-------+-----------+-----+------------------+\n",
      "|2024-01|       Food|  300|             350.0|\n",
      "|2024-02|       Food|  400|             350.0|\n",
      "|2024-03|       Food|  350|             375.0|\n",
      "|2024-01|Electronics| 1000|            1100.0|\n",
      "|2024-02|Electronics| 1200|1033.3333333333333|\n",
      "|2024-03|Electronics|  900|            1050.0|\n",
      "|2024-01|   Clothing|  500|             550.0|\n",
      "|2024-02|   Clothing|  600|             600.0|\n",
      "|2024-03|   Clothing|  700|             650.0|\n",
      "+-------+-----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, lag, lead, sum\n",
    "\n",
    "# Sample sales data\n",
    "sales_data = [\n",
    "    (\"2024-01\", \"Electronics\", 1000),\n",
    "    (\"2024-01\", \"Clothing\", 500),\n",
    "    (\"2024-01\", \"Food\", 300),\n",
    "    (\"2024-02\", \"Electronics\", 1200),\n",
    "    (\"2024-02\", \"Clothing\", 600),\n",
    "    (\"2024-02\", \"Food\", 400),\n",
    "    (\"2024-03\", \"Electronics\", 900),\n",
    "    (\"2024-03\", \"Clothing\", 700),\n",
    "    (\"2024-03\", \"Food\", 350)\n",
    "]\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, [\"month\", \"category\", \"sales\"])\n",
    "\n",
    "sales_df.show()\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(\"month\")\n",
    "\n",
    "# Row number\n",
    "sales_df.withColumn(\"row_num\", row_number().over(window_spec)).show()\n",
    "\n",
    "# Rank (with gaps)\n",
    "rank_window = Window.partitionBy(\"month\").orderBy(desc(\"sales\"))\n",
    "sales_df.withColumn(\"rank\", rank().over(rank_window)).show()\n",
    "\n",
    "# Dense rank (no gaps)\n",
    "sales_df.withColumn(\"dense_rank\", dense_rank().over(rank_window)).show()\n",
    "\n",
    "# Running total\n",
    "sales_df.withColumn(\n",
    "    \"running_total\", \n",
    "    sum(\"sales\").over(window_spec)\n",
    ").show()\n",
    "\n",
    "# Lag and lead (previous and next values)\n",
    "sales_df.select(\n",
    "    col(\"month\"),\n",
    "    col(\"category\"),\n",
    "    col(\"sales\"),\n",
    "    lag(\"sales\", 1).over(window_spec).alias(\"prev_month_sales\"),\n",
    "    lead(\"sales\", 1).over(window_spec).alias(\"next_month_sales\")\n",
    ").show()\n",
    "\n",
    "# Moving average (previous, current, next)\n",
    "moving_window = Window.partitionBy(\"category\") \\\n",
    "    .orderBy(\"month\") \\\n",
    "    .rowsBetween(-1, 1)\n",
    "\n",
    "sales_df.withColumn(\n",
    "    \"moving_avg\",\n",
    "    avg(\"sales\").over(moving_window)\n",
    ").show()\n",
    "\n",
    "# Range-based window (all rows within value range)\n",
    "range_window = Window.partitionBy(\"category\") \\\n",
    "    .orderBy(\"sales\") \\\n",
    "    .rangeBetween(-100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+------+\n",
      "| id|   name| age|salary|\n",
      "+---+-------+----+------+\n",
      "|  1|  Alice|  25| 80000|\n",
      "|  2|    Bob|null| 90000|\n",
      "|  3|Charlie|  35|  null|\n",
      "|  4|   null|  28| 85000|\n",
      "|  5|    Eve|null|  null|\n",
      "+---+-------+----+------+\n",
      "\n",
      "+---+-----+---+------+\n",
      "| id| name|age|salary|\n",
      "+---+-----+---+------+\n",
      "|  1|Alice| 25| 80000|\n",
      "+---+-----+---+------+\n",
      "\n",
      "+---+-------+----+------+\n",
      "| id|   name| age|salary|\n",
      "+---+-------+----+------+\n",
      "|  1|  Alice|  25| 80000|\n",
      "|  2|    Bob|null| 90000|\n",
      "|  3|Charlie|  35|  null|\n",
      "|  4|   null|  28| 85000|\n",
      "|  5|    Eve|null|  null|\n",
      "+---+-------+----+------+\n",
      "\n",
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 80000|\n",
      "|  3|Charlie| 35|  null|\n",
      "|  4|   null| 28| 85000|\n",
      "+---+-------+---+------+\n",
      "\n",
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 80000|\n",
      "|  2|    Bob|  0| 90000|\n",
      "|  3|Charlie| 35|     0|\n",
      "|  4|   null| 28| 85000|\n",
      "|  5|    Eve|  0|     0|\n",
      "+---+-------+---+------+\n",
      "\n",
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 80000|\n",
      "|  2|    Bob|  0| 90000|\n",
      "|  3|Charlie| 35| 50000|\n",
      "|  4|   null| 28| 85000|\n",
      "|  5|    Eve|  0| 50000|\n",
      "+---+-------+---+------+\n",
      "\n",
      "+---+-------+---+------+\n",
      "| id|   name|age|salary|\n",
      "+---+-------+---+------+\n",
      "|  1|  Alice| 25| 80000|\n",
      "|  2|    Bob| 30| 90000|\n",
      "|  3|Charlie| 35|  null|\n",
      "|  4|Unknown| 28| 85000|\n",
      "|  5|    Eve| 30|  null|\n",
      "+---+-------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce, when, isnan\n",
    "\n",
    "# Sample data with nulls\n",
    "data_with_nulls = [\n",
    "    (1, \"Alice\", 25, 80000),\n",
    "    (2, \"Bob\", None, 90000),\n",
    "    (3, \"Charlie\", 35, None),\n",
    "    (4, None, 28, 85000),\n",
    "    (5, \"Eve\", None, None)\n",
    "]\n",
    "\n",
    "null_df = spark.createDataFrame(\n",
    "    data_with_nulls, \n",
    "    [\"id\", \"name\", \"age\", \"salary\"]\n",
    ")\n",
    "\n",
    "null_df.show()\n",
    "\n",
    "# Drop rows with any null values\n",
    "null_df.dropna().show()\n",
    "\n",
    "# Drop rows where all values are null\n",
    "null_df.dropna(how='all').show()\n",
    "\n",
    "# Drop rows with nulls in specific columns\n",
    "null_df.dropna(subset=['age']).show()\n",
    "\n",
    "# Fill null values\n",
    "null_df.fillna(0).show()  # Fill all numeric nulls with 0\n",
    "\n",
    "# Fill with different values for different columns\n",
    "null_df.fillna({'age': 0, 'salary': 50000}).show()\n",
    "\n",
    "# Replace nulls with column default\n",
    "null_df.fillna({'name': 'Unknown', 'age': 30}).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV\n",
    "df.write.mode(\"overwrite\").csv(\"sample_data/output.csv\", header=True)\n",
    "\n",
    "# Write to JSON\n",
    "df.write.mode(\"overwrite\").json(\"sample_data/output.json\")\n",
    "\n",
    "# Write to Parquet (recommended)\n",
    "df.write.mode(\"overwrite\").parquet(\"sample_data/output.parquet\")\n",
    "\n",
    "# Write modes:\n",
    "# - \"overwrite\": Replace existing data\n",
    "# - \"append\": Add to existing data\n",
    "# - \"ignore\": Don't write if data exists\n",
    "# - \"error\" (default): Throw error if data exists\n",
    "\n",
    "# Partitioned write (for big data)\n",
    "df.write.partitionBy(\"department\").parquet(\"sample_data/output_partitioned\")\n",
    "\n",
    "# Write to single file\n",
    "df.coalesce(1).write.csv(\"sample_data/output_single.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+------------------+\n",
      "| department|count|           avg_age|\n",
      "+-----------+-----+------------------+\n",
      "|Engineering|    3|              35.0|\n",
      "|  Marketing|    3|              34.0|\n",
      "|      Sales|    3|32.666666666666664|\n",
      "+-----------+-----+------------------+\n",
      "\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "|      Sales|    2|\n",
      "|Engineering|    3|\n",
      "|  Marketing|    2|\n",
      "+-----------+-----+\n",
      "\n",
      "+---+-------+---+-----------+------+\n",
      "| id|   name|age| department|salary|\n",
      "+---+-------+---+-----------+------+\n",
      "|  1|  Alice| 25|Engineering| 80000|\n",
      "|  2|    Bob| 30|  Marketing| 65000|\n",
      "|  3|Charlie| 35|Engineering| 90000|\n",
      "|  4|  Diana| 28|      Sales| 70000|\n",
      "|  5|    Eve| 32|Engineering| 85000|\n",
      "|  6|  Frank| 45|  Marketing| 75000|\n",
      "|  7|  Grace| 29|      Sales| 72000|\n",
      "|  8|  Henry| 38|Engineering| 95000|\n",
      "|  9|    Ivy| 27|  Marketing| 68000|\n",
      "| 10|   Jack| 41|      Sales| 78000|\n",
      "+---+-------+---+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register DataFrame as temporary view\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "# Run SQL queries\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT department, COUNT(*) as count, AVG(age) as avg_age\n",
    "    FROM employees\n",
    "    WHERE age > 25\n",
    "    GROUP BY department\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "result.show()\n",
    "\n",
    "# Mix SQL and DataFrame API\n",
    "sql_result = spark.sql(\"SELECT * FROM employees WHERE age > 28\")\n",
    "final_result = sql_result.groupBy(\"department\").count()\n",
    "final_result.show()\n",
    "\n",
    "# Register global view (accessible across sessions)\n",
    "df.createOrReplaceGlobalTempView(\"global_employees\")\n",
    "spark.sql(\"SELECT * FROM global_temp.global_employees\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
