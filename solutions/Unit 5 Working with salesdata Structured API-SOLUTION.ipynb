{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with salesdata.txt - Structured API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data in `sales/salesdata.txt` into an DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n",
      "+----------------+----------+-----------+------+----------+\n",
      "|             _c0|       _c1|        _c2|   _c3|       _c4|\n",
      "+----------------+----------+-----------+------+----------+\n",
      "|2023-12-28 10:15|Mondonhedo|Moda hombre|185.82|  metalico|\n",
      "|2022-02-26 19:30| A Corunha|   Farmacia|113.53|mastercard|\n",
      "|2022-10-15 19:15| A Corunha|Moda hombre|341.95|      visa|\n",
      "|2021-07-15 12:47|  Monforte| Automocion|239.83|      visa|\n",
      "|2022-03-09 20:11|   Ourense|    Calzado| 52.98|  efectivo|\n",
      "+----------------+----------+-----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales = spark.read.csv('sales/salesdata.txt', sep='\\t', inferSchema=True)\n",
    "df_sales.printSchema()\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      "\n",
      "+----------------+----------+-----------+------+----------+\n",
      "|             _c0|       _c1|        _c2|   _c3|       _c4|\n",
      "+----------------+----------+-----------+------+----------+\n",
      "|2023-12-28 10:15|Mondonhedo|Moda hombre|185.82|  metalico|\n",
      "|2022-02-26 19:30| A Corunha|   Farmacia|113.53|mastercard|\n",
      "|2022-10-15 19:15| A Corunha|Moda hombre|341.95|      visa|\n",
      "|2021-07-15 12:47|  Monforte| Automocion|239.83|      visa|\n",
      "|2022-03-09 20:11|   Ourense|    Calzado| 52.98|  efectivo|\n",
      "+----------------+----------+-----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales = spark.read \\\n",
    "    .option(\"sep\", '\\t') \\\n",
    "    .csv('sales/salesdata.txt') \n",
    "df_sales.printSchema()\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- cost: double (nullable = true)\n",
      " |-- payment: string (nullable = true)\n",
      "\n",
      "+----------------+----------+-----------+------+----------+\n",
      "|            data|  location|   category|  cost|   payment|\n",
      "+----------------+----------+-----------+------+----------+\n",
      "|2023-12-28 10:15|Mondonhedo|Moda hombre|185.82|  metalico|\n",
      "|2022-02-26 19:30| A Corunha|   Farmacia|113.53|mastercard|\n",
      "|2022-10-15 19:15| A Corunha|Moda hombre|341.95|      visa|\n",
      "|2021-07-15 12:47|  Monforte| Automocion|239.83|      visa|\n",
      "|2022-03-09 20:11|   Ourense|    Calzado| 52.98|  efectivo|\n",
      "+----------------+----------+-----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"data\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"cost\", DoubleType(), True),\n",
    "    StructField(\"payment\", StringType(), True)    \n",
    "])\n",
    "\n",
    "df_sales = spark.read.csv('sales/salesdata.txt', sep='\\t', schema=schema)\n",
    "df_sales.printSchema()\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+------------+------------------+--------+\n",
      "|summary|            data| location|    category|              cost| payment|\n",
      "+-------+----------------+---------+------------+------------------+--------+\n",
      "|  count|        25000000| 25000000|    24916895|          24916895|24833771|\n",
      "|   mean|            null|     null|        null|155.41418828549845|    null|\n",
      "| stddev|            null|     null|        null| 289.1996948916661|    null|\n",
      "|    min|2020-01-01 09:00|A Corunha|Alimentacion|              0.95|   bizum|\n",
      "|    max|2023-12-31 20:59|     Vigo|        Ropa|           3148.63|    visa|\n",
      "+-------+----------------+---------+------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+---------+------------+------------------+--------+\n",
      "|summary|            data| location|    category|              cost| payment|\n",
      "+-------+----------------+---------+------------+------------------+--------+\n",
      "|  count|        25000000| 25000000|    24916895|          24916895|24833771|\n",
      "|   mean|            null|     null|        null|155.41418828549845|    null|\n",
      "| stddev|            null|     null|        null| 289.1996948916662|    null|\n",
      "|    min|2020-01-01 09:00|A Corunha|Alimentacion|              0.95|   bizum|\n",
      "|    25%|            null|     null|        null|             30.19|    null|\n",
      "|    50%|            null|     null|        null|              72.6|    null|\n",
      "|    75%|            null|     null|        null|            160.92|    null|\n",
      "|    max|2023-12-31 20:59|     Vigo|        Ropa|           3148.63|    visa|\n",
      "+-------+----------------+---------+------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----+-------+\n",
      "|data|location|category| cost|payment|\n",
      "+----+--------+--------+-----+-------+\n",
      "|   0|       0|   83105|83105| 166229|\n",
      "+----+--------+--------+-----+-------+\n",
      "\n",
      "data: 1051910\n",
      "location: 20\n",
      "category: 21\n",
      "cost: 275983\n",
      "payment: 10\n"
     ]
    }
   ],
   "source": [
    "# Get count of records\n",
    "df_sales.count()\n",
    "\n",
    "# Check for nulls in each column\n",
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "df_sales.select([count(when(col(c).isNull(), c)).alias(c) for c in df_sales.columns]).show()\n",
    "\n",
    "# Get distinct counts\n",
    "df_sales.select([col(c) for c in df_sales.columns]).distinct().count()\n",
    "\n",
    "# Distinct values per column\n",
    "for column in df_sales.columns:\n",
    "    print(\"{}: {}\".format(\n",
    "        column,\n",
    "        df_sales.select(column).distinct().count()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count the number of sales per Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|  location|  count|\n",
      "+----------+-------+\n",
      "|     Marin|1048705|\n",
      "|    Ferrol|1309293|\n",
      "| A Corunha|1962416|\n",
      "|  Chantada| 784125|\n",
      "|   Ribeira|1308946|\n",
      "|       Foz| 652930|\n",
      "|  Carballo|1309247|\n",
      "|    Sarria| 915957|\n",
      "|   Oleiros|1308475|\n",
      "|      Lugo|1569254|\n",
      "|     Naron|1309011|\n",
      "|   Arteixo|1309270|\n",
      "|Mondonhedo| 656003|\n",
      "|   Ourense|1572445|\n",
      "|      Vigo|1963813|\n",
      "|Pontevedra|1569912|\n",
      "|    Burela| 916669|\n",
      "|    Cangas|1045658|\n",
      "|  Monforte| 786080|\n",
      "|  Santiago|1701791|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy('location').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum the total of sales per Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|    category|           sum(cost)|\n",
      "+------------+--------------------+\n",
      "|    Mascotas| 7.584429399999973E7|\n",
      "|   Cosmetica|1.2452601331999926E8|\n",
      "|  Jardineria|1.0055476798000023E8|\n",
      "|  Moda mujer|2.3703872598999965E8|\n",
      "|        Cine| 1.210281420000001E7|\n",
      "|        null|                null|\n",
      "|      Libros|3.8224972030000106E7|\n",
      "|Complementos|1.0490708236999942E8|\n",
      "|    Deportes|2.1338050501000005E8|\n",
      "|        Ropa|1.6964773458000016E8|\n",
      "| Electronica|1.2624441484899988E9|\n",
      "|      Musica| 5.562587021999992E7|\n",
      "|  Ferreteria|1.3911944093000016E8|\n",
      "|     Calzado| 1.629241953400009E8|\n",
      "|       Bebes|1.6751150133999974E8|\n",
      "|  Automocion| 4.095095070099965E8|\n",
      "|Alimentacion|2.0506244590000074E7|\n",
      "|    Farmacia|  5.14402004999998E7|\n",
      "| Moda hombre|1.9768038597999948E8|\n",
      "|    Juguetes| 8.520542355000019E7|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.groupBy('category').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the minimum sale per date (ordered by date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales.createOrReplaceView(\"tiendas\")\n",
    "\n",
    "spark.sql('select date ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|min(cost)|\n",
      "+----------+---------+\n",
      "|2020-01-01|      1.0|\n",
      "|2020-01-02|     1.17|\n",
      "|2020-01-03|     1.01|\n",
      "|2020-01-04|     1.01|\n",
      "|2020-01-05|     1.04|\n",
      "|2020-01-06|      1.0|\n",
      "|2020-01-07|      1.0|\n",
      "|2020-01-08|     1.05|\n",
      "|2020-01-09|     0.96|\n",
      "|2020-01-10|     1.04|\n",
      "|2020-01-11|     1.02|\n",
      "|2020-01-12|     1.03|\n",
      "|2020-01-13|     1.06|\n",
      "|2020-01-14|     1.05|\n",
      "|2020-01-15|     1.01|\n",
      "|2020-01-16|     1.03|\n",
      "|2020-01-17|     0.99|\n",
      "|2020-01-18|     1.03|\n",
      "|2020-01-19|     1.01|\n",
      "|2020-01-20|     0.97|\n",
      "+----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "df_sales.withColumn('date', substring(\"data\",1,10)).groupBy('date').min().orderBy('date',ascending=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate de average of sales per category when payment method is cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      payment|\n",
      "+-------------+\n",
      "|       paypal|\n",
      "|         null|\n",
      "|         cash|\n",
      "|       cheque|\n",
      "|transferencia|\n",
      "|        bizum|\n",
      "|   mastercard|\n",
      "|     metalico|\n",
      "|         visa|\n",
      "|     efectivo|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.select('payment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|    category|          avg_cost|\n",
      "+------------+------------------+\n",
      "|        Cine| 9.725644451664005|\n",
      "|Alimentacion|16.383041183827473|\n",
      "|      Libros| 27.18371338166287|\n",
      "|    Farmacia|34.395888713329484|\n",
      "|      Musica| 37.99189976035834|\n",
      "|    Mascotas|  49.4886747817107|\n",
      "|    Juguetes| 56.12774776663495|\n",
      "|  Jardineria| 65.41876226556793|\n",
      "|Complementos| 69.54259463496108|\n",
      "|  Ferreteria| 80.41413392156794|\n",
      "|   Cosmetica|  82.7454321649516|\n",
      "|       Bebes| 92.12014584545565|\n",
      "|        Ropa| 94.59480219691577|\n",
      "|       Hogar|106.83470334196866|\n",
      "|     Calzado|106.97273763555948|\n",
      "|    Deportes|107.08177714089662|\n",
      "| Moda hombre|108.04427212805584|\n",
      "|  Moda mujer|116.72306091331262|\n",
      "|  Automocion|148.16625218783474|\n",
      "| Electronica| 200.0971840695597|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales.filter(df_sales.payment.isin(['cash', 'metalico', 'efectivo'])) \\\n",
    "    .groupBy('category') \\\n",
    "    .agg(avg('cost').alias('avg_cost')) \\\n",
    "    .orderBy('avg_cost') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
